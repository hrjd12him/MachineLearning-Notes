{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 and L2 regularization are two common techniques used in machine learning and statistics to prevent overfitting in models, particularly in the context of linear models like linear regression and logistic regression. They add a penalty term to the loss function that the model is trying to optimize, encouraging the model to have smaller coefficients for the input features. Here's a brief overview of L1 and L2 regularization:\n",
    "\n",
    "1. **L1 Regularization (Lasso Regression):**\n",
    "   - L1 regularization adds the absolute values of the coefficients (weights) as a penalty to the loss function.\n",
    "   - It's also known as Lasso (Least Absolute Shrinkage and Selection Operator) regression.\n",
    "   - L1 regularization encourages sparsity in the model, meaning it tends to force some of the coefficients to be exactly zero, effectively performing feature selection.\n",
    "   - It is useful when you suspect that many features are irrelevant or redundant.\n",
    "\n",
    "   The L1 regularization term is added to the loss function as follows:\n",
    "   Loss with L1 regularization = Loss without regularization + λ * Σ|βi|, where βi represents the model coefficients.\n",
    "\n",
    "2. **L2 Regularization (Ridge Regression):**\n",
    "   - L2 regularization adds the squared values of the coefficients as a penalty to the loss function.\n",
    "   - It's also known as Ridge regression.\n",
    "   - L2 regularization encourages the coefficients to be small but doesn't force them to be exactly zero. It's helpful for avoiding very large weights.\n",
    "\n",
    "   The L2 regularization term is added to the loss function as follows:\n",
    "   Loss with L2 regularization = Loss without regularization + λ * Σ(βi^2), where βi represents the model coefficients.\n",
    "\n",
    "The key difference between L1 and L2 regularization is in the way they penalize coefficients:\n",
    "\n",
    "- L1 encourages sparsity and can perform feature selection by driving some coefficients to zero.\n",
    "- L2 encourages small coefficients but doesn't typically force them to be exactly zero.\n",
    "\n",
    "Both L1 and L2 regularization help in preventing overfitting by adding a constraint on the magnitude of the model coefficients, making the model more generalizable to new data. The parameter λ (lambda) controls the strength of the regularization. A higher λ value leads to stronger regularization.\n",
    "\n",
    "In practice, a combination of both L1 and L2 regularization, called Elastic Net regularization, is often used to take advantage of the benefits of both methods while avoiding their drawbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
